# @package training

# Training hyperparameters for 512x512 images
batch_size: 16  # Smaller batch size for larger images
learning_rate: 5.0e-5  # Lower learning rate for stability
weight_decay: 0.0
beta1: 0.9
beta2: 0.999
eps: 1.0e-8

# Training schedule
warmup_steps: 20000  # Longer warmup for larger images
max_steps: 2000000
num_epochs: 100
grad_clip_norm: 1.0

# EMA
ema_decay: 0.9999
ema_update_freq: 1

# Data
image_size: 512
num_classes: 1000

# Optimizer
optimizer:
  _target_: torch.optim.AdamW
  lr: ${training.learning_rate}
  weight_decay: ${training.weight_decay}
  betas: [${training.beta1}, ${training.beta2}]
  eps: ${training.eps}

# Default configurations (can be overridden)
defaults:
  - override /scheduler: cosine
  - override /loss: dispersive_default
  - override /dataset: imagenet_512
